{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limited Feature Random Forest Notebook\n",
    "## Adapted from model_training.ipynb\n",
    "\n",
    "This runs most of the same code as in model_training.ipynb but with the bulk of it put into functions for easier reusing. Then a new random forest PKL file is created taking the top ten most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) \n",
    "\n",
    "claims_data = pd.read_csv('ProcessedClaimData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\" + \"=\"*50)\n",
    "print(\"ANALYZING DUPLICATES IN THE DATASET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check for complete duplicates\n",
    "duplicate_rows = claims_data.duplicated().sum()\n",
    "print(f\"Number of complete duplicate rows: {duplicate_rows}\")\n",
    "\n",
    "# Analyze settlement value distribution\n",
    "settlement_counts = claims_data['SettlementValue'].value_counts().head(10)\n",
    "print(\"\\nTop 10 most frequent settlement values:\")\n",
    "print(settlement_counts)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "settlement_counts.plot(kind='bar')\n",
    "plt.title('Most Common Settlement Values')\n",
    "plt.xlabel('Settlement Value (£)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for partial duplicates in key feature combinations\n",
    "print(\"\\npotential pattern duplicates\")\n",
    "key_features = ['AccidentType', 'InjuryPrognosis', 'DominantInjury', 'VehicleType']\n",
    "pattern_dupes = claims_data.duplicated(subset=key_features).sum()\n",
    "print(f\"Rows with duplicate patterns across key features: {pattern_dupes}\")\n",
    "\n",
    "if pattern_dupes > 0:\n",
    "    # Show the most common patterns\n",
    "    pattern_counts = claims_data.groupby(key_features).size().sort_values(ascending=False).head(10)\n",
    "    print(\"\\nMost common claim patterns:\")\n",
    "    print(pattern_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_columns(claims_data):\n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_cols = claims_data.select_dtypes(include=['object', 'string']).columns.tolist()\n",
    "    numerical_cols = claims_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "    # Remove the target variable from the feature lists\n",
    "    target_col = 'SettlementValue'\n",
    "\n",
    "    # Remove both the log-transformed target AND the original settlement value from features\n",
    "    if target_col in numerical_cols:\n",
    "        numerical_cols.remove(target_col)\n",
    "    \n",
    "    # adding it later\n",
    "    if 'LogSettlementValue' in numerical_cols:\n",
    "        numerical_cols.remove('LogSettlementValue')\n",
    "\n",
    "    print(f\"\\nCategorical columns: {categorical_cols}\")\n",
    "    print(f\"Numerical columns: {numerical_cols}\")\n",
    "    print(f\"Target column: {target_col}\")\n",
    "\n",
    "    return categorical_cols, numerical_cols, target_col\n",
    "\n",
    "def define_preprocessor(X_train):\n",
    "    categorical_cols, numerical_cols, _ = identify_columns(claims_data=claims_data)\n",
    "\n",
    "    categorical_cols = [col for col in categorical_cols if col in X_train.columns]\n",
    "    numerical_cols = [col for col in numerical_cols if col in X_train.columns]\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "    return preprocessor, categorical_cols, numerical_cols\n",
    "\n",
    "# Function to evaluate a model\n",
    "def evaluate_model(model, X, y, model_name, log_transform=True):\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Calculate metrics on log scale\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    # Calculate metrics on original GBP scale\n",
    "    if log_transform:\n",
    "        y_pounds = np.expm1(y)\n",
    "        y_pred_pounds = np.expm1(y_pred)\n",
    "        mse_pounds = mean_squared_error(y_pounds, y_pred_pounds)\n",
    "        rmse_pounds = np.sqrt(mse_pounds)\n",
    "        mae_pounds = mean_absolute_error(y_pounds, y_pred_pounds)\n",
    "    else:\n",
    "        y_pounds = y\n",
    "        y_pred_pounds = y_pred\n",
    "        mse_pounds = mse\n",
    "        rmse_pounds = rmse\n",
    "        mae_pounds = mae\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R^2': r2,\n",
    "        'Name': model_name,\n",
    "        'MSE_GBP': mse_pounds,\n",
    "        'RMSE_GBP': rmse_pounds,\n",
    "        'MAE_GBP': mae_pounds,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_GBP': y_pred_pounds\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_pipeline(models, preprocesser, X_train, y_train, X_val, y_val):\n",
    "    # Dictionary to store model results\n",
    "    model_results = {}\n",
    "    print(\"\\nTraining and evaluating models:\")\n",
    "\n",
    "    for name, model in models.items():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create pipeline with preprocessing and model\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocesser),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        \n",
    "        # Train the model\n",
    "        print(f\"Training {name}...\")\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        results = evaluate_model(pipeline, X_val, y_val, name)\n",
    "        results['Training Time'] = time.time() - start_time\n",
    "        model_results[name] = results\n",
    "        \n",
    "        print(f\"  {name} - RMSE: {results['RMSE']:.2f}, R^2: {results['R^2']:.4f}, Time: {results['Training Time']:.2f}s\")\n",
    "        print(f\"  Error in GBP - RMSE: £{results['RMSE_GBP']:.2f}, MAE: £{results['MAE_GBP']:.2f}\")\n",
    "        \n",
    "        # Store the pipeline\n",
    "        model_results[name]['pipeline'] = pipeline\n",
    "\n",
    "    # Using RMSE to compare the models\n",
    "    best_model_name = min(model_results, key=lambda k: model_results[k]['RMSE'])\n",
    "    print(f\"\\nBest model based on validation RMSE: {best_model_name}\")\n",
    "    best_pipeline = model_results[best_model_name]['pipeline']\n",
    "\n",
    "    return best_model_name, best_pipeline, model_results\n",
    "\n",
    "def model_tuner(best_pipeline, param_grid, model_results, X_train, y_train, X_val, y_val):\n",
    "    # Only performs hyperparameter tuning if we have parameters to tune\n",
    "    if param_grid:\n",
    "        best_model_name = best_pipeline.named_steps['model'].__class__.__name__\n",
    "        \n",
    "        print(f\"\\nPerforming hyperparameter tuning for {best_model_name}...\")\n",
    "        \n",
    "        # Cross validation \n",
    "        grid_search = GridSearchCV(\n",
    "            best_pipeline,\n",
    "            param_grid,\n",
    "            cv=5,\n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        best_pipeline = grid_search.best_estimator_\n",
    "        \n",
    "        tuned_results = evaluate_model(best_pipeline, X_val, y_val, f\"{best_model_name} (Tuned)\")\n",
    "        print(f\"Tuned model - RMSE: {tuned_results['RMSE']:.2f}, R^2: {tuned_results['R^2']:.4f}\")\n",
    "        print(f\"Error in GBP - RMSE: £{tuned_results['RMSE_GBP']:.2f}, MAE: £{tuned_results['MAE_GBP']:.2f}\")\n",
    "        \n",
    "        model_results[f\"{best_model_name} (Tuned)\"] = tuned_results\n",
    "        model_results[f\"{best_model_name} (Tuned)\"][\"pipeline\"] = best_pipeline\n",
    "        \n",
    "        if tuned_results['RMSE'] < model_results[best_model_name]['RMSE']:\n",
    "            best_model_name = f\"{best_model_name} (Tuned)\"\n",
    "\n",
    "    return best_pipeline, best_model_name, model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'AccidentDate' in claims_data.columns and 'ClaimDate' in claims_data.columns:\n",
    "    claims_data['DaysBetweenAccidentAndClaim'] = claims_data['ClaimDate'] - claims_data['AccidentDate']\n",
    "\n",
    "categorical_cols, numerical_cols, target_col = identify_columns(claims_data=claims_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "claims_data['SettlementValue'] = np.log1p(claims_data[target_col])\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(claims_data['SettlementValue'], kde=True)\n",
    "plt.title('Distribution of Transformed Settlement Values')\n",
    "plt.xlabel('Settlement Value)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = claims_data[target_col].skew()\n",
    "print(f\"Skewness of target variable: {skewness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transform = False\n",
    "if abs(skewness) > 1:\n",
    "    log_transform = True\n",
    "    claims_data['LogSettlementValue'] = np.log1p(claims_data[target_col])\n",
    "    target_col= 'LogSettlementValue'\n",
    "    \n",
    "    sns.histplot(claims_data['LogSettlementValue'], kde=True)\n",
    "    plt.title('Log-Transformed Settlement Values')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nApplied log transformation due to skewness. New target column: {target_col}\")\n",
    "    skewness = claims_data[target_col].skew()\n",
    "    print(f\"Skewness of target variable: {skewness}\")\n",
    "else:\n",
    "    target_col = target_col\n",
    "    print(\"\\nTarget variable is not highly skewed, using original values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting data\n",
    "X = claims_data.drop(columns=[target_col])\n",
    "if 'SettlementValue' in X.columns:\n",
    "    X = X.drop(columns=['SettlementValue'])\n",
    "    \n",
    "y = claims_data[target_col]\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set size: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of models to try\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'RandomForestRegressor': RandomForestRegressor(random_state=42),\n",
    "    'MLPRegressor': MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "}\n",
    "\n",
    "preprocessor_tuple = define_preprocessor(claims_data)\n",
    "preprocessor_obj = preprocessor_tuple[0]  # Extract just the preprocessor object\n",
    "\n",
    "best_model_name, best_pipeline, model_results = execute_pipeline(\n",
    "    models=models, \n",
    "    preprocesser=preprocessor_obj,  # Pass just the preprocessor object\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val\n",
    ")\n",
    "# Tuning the best hyperparameter Tuning for the Best Model\n",
    "param_grid = {}\n",
    "\n",
    "if best_model_name == 'LinearRegression':\n",
    "    # regression model does not have hyperparameters to tune but best to keep this to keep track \n",
    "    pass\n",
    "elif best_model_name == 'Ridge':\n",
    "    param_grid = {\n",
    "        'model__alpha': [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "    }\n",
    "elif best_model_name == 'RandomForestRegressor':\n",
    "    param_grid = {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__max_depth': [None, 10, 20],\n",
    "        'model__min_samples_split': [2, 5],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline, best_model_name, model_results = model_tuner(\n",
    "    best_pipeline, \n",
    "    param_grid, \n",
    "    model_results,\n",
    "    X_train,\n",
    "    y_train,  \n",
    "    X_val,    \n",
    "    y_val     \n",
    ")\n",
    "# Evaluate the best model on the test set\n",
    "best_pipeline = model_results[best_model_name]['pipeline']\n",
    "test_results = evaluate_model(best_pipeline, X_test, y_test, best_model_name)\n",
    "print(f\"\\nTest set evaluation for {best_model_name}:\")\n",
    "print(f\"  RMSE: {test_results['RMSE']:.2f}\")\n",
    "print(f\"  MAE: {test_results['MAE']:.2f}\")\n",
    "print(f\"  R^2: {test_results['R^2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_settlement_plot(y_pred_test, y_test, best_model_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred_test, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Actual Settlement Values')\n",
    "    plt.ylabel('Predicted Settlement Values')\n",
    "    plt.title(f'Actual vs. Predicted Settlement Values ({best_model_name})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def residual_plot(y_test, y_pred_test):\n",
    "    # Calculate residuals\n",
    "    residuals = y_test - y_pred_test\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_pred_test, residuals, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted Settlement Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residual Plot')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = best_pipeline.predict(X_test)\n",
    "\n",
    "predicted_settlement_plot(y_test=y_test, y_pred_test=y_pred_test, best_model_name=best_model_name)\n",
    "residual_plot(y_test=y_test, y_pred_test=y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def important_features_calc(best_pipeline):\n",
    "    # Get feature names after preprocessing\n",
    "    if categorical_cols:\n",
    "        # Get feature names from the preprocessor\n",
    "        preprocessor = best_pipeline.named_steps['preprocessor']\n",
    "        feature_names = []\n",
    "        \n",
    "        # Get numerical feature names\n",
    "        if numerical_cols:\n",
    "            feature_names.extend(numerical_cols)\n",
    "        \n",
    "        # Get one-hot encoded feature names\n",
    "        if categorical_cols:\n",
    "            ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "            # Get category names from one-hot encoder\n",
    "            cat_features = ohe.get_feature_names_out(categorical_cols)\n",
    "            feature_names.extend(cat_features)\n",
    "    else:\n",
    "        feature_names = numerical_cols\n",
    "\n",
    "    # For tree-based models, we can get feature importance directly\n",
    "    if hasattr(best_pipeline.named_steps['model'], 'feature_importances_'):\n",
    "        importances = best_pipeline.named_steps['model'].feature_importances_\n",
    "        feature_imp = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_imp.head(20))\n",
    "        plt.title(f'Top 20 Feature Importances ({best_model_name})')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nTop 10 important features:\")\n",
    "        print(feature_imp.head(10))\n",
    "    else:\n",
    "        # For linear models, use coefficients\n",
    "        if hasattr(best_pipeline.named_steps['model'], 'coef_'):\n",
    "            coefs = best_pipeline.named_steps['model'].coef_\n",
    "            \n",
    "            if coefs.ndim == 1:  # For models with a 1D coefficient array\n",
    "                feature_imp = pd.DataFrame({\n",
    "                    'Feature': feature_names,\n",
    "                    'Coefficient': coefs\n",
    "                }).sort_values('Coefficient', key=abs, ascending=False)\n",
    "                \n",
    "                # Plot coefficient values\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                sns.barplot(x='Coefficient', y='Feature', data=feature_imp.head(20))\n",
    "                plt.title(f'Top 20 Feature Coefficients ({best_model_name})')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                print(\"\\nTop 10 features by coefficient magnitude:\")\n",
    "                print(feature_imp.head(10))\n",
    "        else:\n",
    "            # Permutation importance for models without built-in feature importance\n",
    "            print(\"\\nCalculating permutation importance...\")\n",
    "            perm_importance = permutation_importance(\n",
    "                best_pipeline, X_val, y_val, n_repeats=10, random_state=42, n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            feature_imp = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': perm_importance.importances_mean\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    return feature_imp\n",
    "\n",
    "def permutation_importance_plotting(feature_imp):\n",
    "    # Plot permutation importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_imp.head(20))\n",
    "    plt.title(f'Top 20 Permutation Importances ({best_model_name})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nTop 10 features by permutation importance:\")\n",
    "    print(feature_imp.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = important_features_calc(best_pipeline=best_pipeline)\n",
    "permutation_importance_plotting(feature_imp=feature_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_bias_plot(X_test_with_preds):\n",
    "    # Check for biases in predictions by protected characteristics\n",
    "    if 'Gender' in X_test.columns:\n",
    "        # Calculate average error by gender\n",
    "        gender_errors = X_test_with_preds.groupby('Gender')['Error'].mean()\n",
    "        \n",
    "        print(\"\\nAverage prediction error by gender:\")\n",
    "        print(gender_errors)\n",
    "\n",
    "        # Plot gender error comparison\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        gender_errors.plot(kind='bar')\n",
    "        plt.title('Average Prediction Error by Gender')\n",
    "        plt.ylabel('Mean Absolute Error')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Gender Column not found!\")\n",
    "    \n",
    "def age_bias_plot(X_test_with_preds):\n",
    "    # Check for biases by age\n",
    "    if 'DriverAge' in X_test.columns:\n",
    "        # Create age groups\n",
    "        X_test_with_preds['AgeGroup'] = pd.cut(X_test_with_preds['DriverAge'], \n",
    "                                            bins=[0, 30, 40, 50, 60, 70, 80],\n",
    "                                            labels=['Under 30', '30-40', '40-50', '50-60', '60-70', '70-80'])\n",
    "        \n",
    "        # Calculate average error by age group\n",
    "        age_group_errors = X_test_with_preds.groupby('AgeGroup', observed=False)['Error'].mean()\n",
    "\n",
    "        print(\"\\nAverage prediction error by age group:\")\n",
    "        print(age_group_errors)\n",
    "\n",
    "        # Plot age group error comparison\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        age_group_errors.plot(kind='bar')\n",
    "        plt.title('Average Prediction Error by Age Group')\n",
    "        plt.ylabel('Mean Absolute Error')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Age Column not found\")\n",
    "\n",
    "def bias_helper(X_test, y_test, y_pred_test):\n",
    "    # Add predictions to test data\n",
    "    X_test_with_preds = X_test.copy()\n",
    "    X_test_with_preds['Predicted'] = y_pred_test\n",
    "    X_test_with_preds['Actual'] = y_test.values\n",
    "    X_test_with_preds['Error'] = abs(y_test.values - y_pred_test)\n",
    "\n",
    "    return X_test_with_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_test_with_preds = bias_helper(X_test=X_test, y_test=y_test, y_pred_test=y_pred_test)\n",
    "\n",
    "age_bias_plot(X_test_with_preds=X_test_with_preds)\n",
    "gender_bias_plot(X_test_with_preds=X_test_with_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Random Forest with only the top ten most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = list(feature_imp.head(10)['Feature'])\n",
    "\n",
    "X_train_filtered = X_train[top_10]\n",
    "\n",
    "preprocessor = define_preprocessor(X_train_filtered)\n",
    "\n",
    "models = {'RandomForestRegressor': RandomForestRegressor(random_state=42),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all features have transferred over\n",
    "missing_columns = set(top_10) - set(X_train_filtered.columns)\n",
    "if missing_columns:\n",
    "    print(f\"Missing columns: {missing_columns}\")\n",
    "else:\n",
    "    print(\"All top 10 features are present in the filtered dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_tuple = define_preprocessor(claims_data)\n",
    "preprocessor_obj = preprocessor_tuple[0]  \n",
    "\n",
    "best_model_name, best_pipeline, model_results = execute_pipeline(\n",
    "    models=models, \n",
    "    preprocesser=preprocessor_obj,  # Pass just the preprocessor object\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "        'model__n_estimators': [100, 200, 300, 400, 500],\n",
    "        'model__max_depth': [None, 10, 20, 30, 50],\n",
    "        'model__min_samples_split': [2, 5],\n",
    "    }\n",
    "\n",
    "best_pipeline, best_model_name, model_results = model_tuner(\n",
    "    best_pipeline, \n",
    "    param_grid, \n",
    "    model_results,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,  \n",
    "    y_val   \n",
    ")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_pipeline = model_results[best_model_name]['pipeline']\n",
    "test_results = evaluate_model(best_pipeline, X_test, y_test, best_model_name)\n",
    "print(f\"\\nTest set evaluation for {best_model_name}:\")\n",
    "print(f\"  RMSE: {test_results['RMSE']:.2f}\")\n",
    "print(f\"  MAE: {test_results['MAE']:.2f}\")\n",
    "print(f\"  R^2: {test_results['R^2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tree models, use TreeExplainer; for others fall back to KernelExplainer.\n",
    "model = best_pipeline.named_steps['model']\n",
    "preprocessor = best_pipeline.named_steps['preprocessor']\n",
    "\n",
    "# Prepare a subset of validation data for explanation\n",
    "X_val_sample = X_val.sample(100, random_state=42)\n",
    "X_val_processed = preprocessor.transform(X_val_sample)\n",
    "\n",
    "if hasattr(model, 'feature_importances_'):\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "else:\n",
    "    explainer = shap.KernelExplainer(model.predict, X_val_processed, link=\"logit\")\n",
    "    \n",
    "# Compute SHAP values (be cautious with KernelExplainer as it can be slow)\n",
    "shap_values = explainer.shap_values(X_val_processed)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "if hasattr(preprocessor, 'get_feature_names_out'):\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "else:\n",
    "    feature_names = X_val.columns\n",
    "\n",
    "# Plot the summary plot\n",
    "shap.summary_plot(shap_values, X_val_processed, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with actual values, predictions, and error in GBP (expm1 to reverse log transform)\n",
    "X_test_with_preds = bias_helper(X_test=X_test, y_test=y_test, y_pred_test=y_pred_test)\n",
    "X_test_with_preds['Actual_GBP'] = np.expm1(X_test_with_preds['Actual'])\n",
    "X_test_with_preds['Predicted_GBP'] = np.expm1(X_test_with_preds['Predicted'])\n",
    "X_test_with_preds['Error_GBP'] = abs(X_test_with_preds['Actual_GBP'] - X_test_with_preds['Predicted_GBP'])\n",
    "\n",
    "# Use a simple clustering on the prediction and error space.\n",
    "clustering_features = X_test_with_preds[['Predicted_GBP', 'Error_GBP']]\n",
    "\n",
    "# Run KMeans clustering to identify 3 groups (you can change the number of clusters as needed)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "X_test_with_preds['Cluster'] = kmeans.fit_predict(clustering_features)\n",
    "\n",
    "# Plot the scatter of Actual vs Predicted claim values, colored by cluster assignment\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=X_test_with_preds, x='Actual_GBP', y='Predicted_GBP', hue='Cluster', palette='viridis', alpha=0.7)\n",
    "plt.plot([X_test_with_preds['Actual_GBP'].min(), X_test_with_preds['Actual_GBP'].max()],\n",
    "         [X_test_with_preds['Actual_GBP'].min(), X_test_with_preds['Actual_GBP'].max()],\n",
    "         'r--', label='Ideal Prediction')\n",
    "plt.xlabel('Actual Settlement Value (£)')\n",
    "plt.ylabel('Predicted Settlement Value (£)')\n",
    "plt.title('Scatter Plot of Actual vs. Predicted Settlement Values by Cluster')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Report average error in each cluster\n",
    "cluster_error = X_test_with_preds.groupby('Cluster')['Error_GBP'].mean().reset_index()\n",
    "print(\"Average Prediction Error (in £) by Cluster:\")\n",
    "print(cluster_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe to analyze prediction accuracy\n",
    "accuracy_analysis = X_test_with_preds.copy()\n",
    "\n",
    "# Calculate the difference (actual - predicted)\n",
    "accuracy_analysis['Difference'] = accuracy_analysis['Actual'] - accuracy_analysis['Predicted']\n",
    "\n",
    "# Categorize predictions\n",
    "accuracy_analysis['Prediction_Category'] = pd.cut(\n",
    "    accuracy_analysis['Difference'],\n",
    "    bins=[-float('inf'), -0.1, 0.1, float('inf')],\n",
    "    labels=['Overestimated', 'Accurate', 'Underestimated']\n",
    ")\n",
    "\n",
    "# Count the number of predictions in each category\n",
    "category_counts = accuracy_analysis['Prediction_Category'].value_counts()\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = category_counts.plot(kind='bar', color=['#ff9999', '#66b3ff', '#99ff99'])\n",
    "plt.title('Distribution of Prediction Accuracy')\n",
    "plt.xlabel('Prediction Category')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add count labels on top of each bar\n",
    "for i, v in enumerate(category_counts):\n",
    "    plt.text(i, v + 5, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print percentage breakdown\n",
    "percentages = (category_counts / len(accuracy_analysis) * 100).round(1)\n",
    "print(\"Percentage of predictions by category:\")\n",
    "for category, percentage in percentages.items():\n",
    "    print(f\"{category}: {percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between predicted and actual values in GBP\n",
    "X_test_with_preds['Difference_GBP'] = X_test_with_preds['Predicted_GBP'] - X_test_with_preds['Actual_GBP']\n",
    "\n",
    "# Create a histogram of the differences\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(X_test_with_preds['Difference_GBP'], bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Prediction Error (Predicted - Actual in £)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Errors in GBP')\n",
    "\n",
    "# Add statistics to the plot\n",
    "mean_error = X_test_with_preds['Difference_GBP'].mean()\n",
    "median_error = X_test_with_preds['Difference_GBP'].median()\n",
    "std_error = X_test_with_preds['Difference_GBP'].std()\n",
    "\n",
    "stats_text = f'Mean Error: £{mean_error:.2f}\\nMedian Error: £{median_error:.2f}\\nStd Deviation: £{std_error:.2f}'\n",
    "plt.annotate(stats_text, xy=(0.75, 0.8), xycoords='axes fraction', \n",
    "             bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desd_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
