{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) \n",
    "\n",
    "claims_data = pd.read_csv('ProcessedClaimDataLog.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_columns(claims_data):\n",
    "\n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_cols = claims_data.select_dtypes(include=['object', 'string']).columns.tolist()\n",
    "    numerical_cols = claims_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "    # Remove the target variable from the feature lists\n",
    "    target_col = 'LogSettlementValue'\n",
    "\n",
    "    # Remove both the log-transformed target AND the original settlement value from features\n",
    "    if target_col in numerical_cols:\n",
    "        numerical_cols.remove(target_col)\n",
    "    if 'SettlementValue' in numerical_cols: \n",
    "        numerical_cols.remove('SettlementValue') \n",
    "\n",
    "    print(f\"\\nCategorical columns: {categorical_cols}\")\n",
    "    print(f\"Numerical columns: {numerical_cols}\")\n",
    "    print(f\"Target column: {target_col}\")\n",
    "\n",
    "    return categorical_cols, numerical_cols, target_col\n",
    "\n",
    "# Update categorical and numerical columns based on the transformed dataset\n",
    "def define_preprocessor(X_train):\n",
    "    categorical_cols, numerical_cols, _ = identify_columns(claims_data=claims_data)\n",
    "\n",
    "    categorical_cols = [col for col in categorical_cols if col in X_train.columns]\n",
    "    numerical_cols = [col for col in numerical_cols if col in X_train.columns]\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "# Function to evaluate a model\n",
    "def evaluate_model(model, X, y, model_name):\n",
    "    y_pred = model.predict(X)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R^2': r2,\n",
    "        'Name': model_name\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(claimData['SettlementValue'], bins=50, kde=True)\n",
    "plt.title('Distribution of Settlement Values')\n",
    "plt.xlabel('Settlement Value (£)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(x=520, color='r', linestyle='--', label='£520')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create a version of the dataset with fewer £520 values to test impact\n",
    "high_freq_value = 520\n",
    "# Identify indices of rows with settlement value of £520\n",
    "high_freq_indices = claimData[claimData['SettlementValue'] == high_freq_value].index\n",
    "# Randomly select half of these indices\n",
    "np.random.seed(42)\n",
    "remove_indices = np.random.choice(high_freq_indices, size=len(high_freq_indices)//2, replace=False)\n",
    "\n",
    "balanced_claims = claimData.drop(remove_indices)\n",
    "\n",
    "print(f\"Original dataset size: {len(claimData)}\")\n",
    "print(f\"Balanced dataset size: {len(balanced_claims)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limited Feature Random Forest Notebook\n",
    "## Adapted from model_training.ipynb\n",
    "\n",
    "This runs most of the same code as in model_training.ipynb but with the bulk of it put into functions for easier reusing. Then a new random forest PKL file is created taking the top ten most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) \n",
    "\n",
    "claims_data = pd.read_csv('ProcessedClaimDataLog.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_columns(claims_data):\n",
    "\n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_cols = claims_data.select_dtypes(include=['object', 'string']).columns.tolist()\n",
    "    numerical_cols = claims_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "    # Remove the target variable from the feature lists\n",
    "    target_col = 'LogSettlementValue'\n",
    "\n",
    "    # Remove both the log-transformed target AND the original settlement value from features\n",
    "    if target_col in numerical_cols:\n",
    "        numerical_cols.remove(target_col)\n",
    "    if 'SettlementValue' in numerical_cols: \n",
    "        numerical_cols.remove('SettlementValue') \n",
    "\n",
    "    print(f\"\\nCategorical columns: {categorical_cols}\")\n",
    "    print(f\"Numerical columns: {numerical_cols}\")\n",
    "    print(f\"Target column: {target_col}\")\n",
    "\n",
    "    return categorical_cols, numerical_cols, target_col\n",
    "\n",
    "# Update categorical and numerical columns based on the transformed dataset\n",
    "def define_preprocessor(X_train):\n",
    "    categorical_cols, numerical_cols, _ = identify_columns(claims_data=claims_data)\n",
    "\n",
    "    categorical_cols = [col for col in categorical_cols if col in X_train.columns]\n",
    "    numerical_cols = [col for col in numerical_cols if col in X_train.columns]\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "# Function to evaluate a model\n",
    "def evaluate_model(model, X, y, model_name):\n",
    "    y_pred = model.predict(X)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R^2': r2,\n",
    "        'Name': model_name\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'AccidentDate' in claims_data.columns and 'ClaimDate' in claims_data.columns:\n",
    "    claims_data['DaysBetweenAccidentAndClaim'] = claims_data['ClaimDate'] - claims_data['AccidentDate']\n",
    "\n",
    "categorical_cols, numerical_cols, target_col = identify_columns(claims_data=claims_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "X = claims_data.drop(columns=[target_col])\n",
    "y = claims_data[target_col]\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set size: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_pipeline(models, preprocesser):\n",
    "    # Dictionary to store model results\n",
    "    model_results = {}\n",
    "    print(\"\\nTraining and evaluating models:\")\n",
    "\n",
    "    for name, model in models.items():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create pipeline with preprocessing and model\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocesser),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        \n",
    "        # Train the model\n",
    "        print(f\"Training {name}...\")\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        results = evaluate_model(pipeline, X_val, y_val, name)\n",
    "        results['Training Time'] = time.time() - start_time\n",
    "        model_results[name] = results\n",
    "        \n",
    "        print(f\"  {name} - RMSE: {results['RMSE']:.2f}, R^2: {results['R^2']:.4f}, Time: {results['Training Time']:.2f}s\")\n",
    "        \n",
    "        # Store the pipeline\n",
    "        model_results[name]['pipeline'] = pipeline\n",
    "\n",
    "    # using RSMe to compare the models\n",
    "    best_model_name = min(model_results, key=lambda k: model_results[k]['RMSE'])\n",
    "    print(f\"\\nBest model based on validation RMSE: {best_model_name}\")\n",
    "    best_pipeline = model_results[best_model_name]['pipeline']\n",
    "\n",
    "    return best_model_name, best_pipeline, model_results\n",
    "\n",
    "def model_tuner(best_pipeline, param_grid, model_results):\n",
    "    # only performs hyperparameter tuning if we have parameters to tune\n",
    "    if param_grid:\n",
    "        best_model_name = best_pipeline['model'].__class__.__name__\n",
    "\n",
    "        print(f\"\\nPerforming hyperparameter tuning for {best_model_name}...\")\n",
    "        \n",
    "        # cross validation \n",
    "        grid_search = GridSearchCV(\n",
    "            best_pipeline,\n",
    "            param_grid,\n",
    "            cv=5,\n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        best_pipeline = grid_search.best_estimator_\n",
    "        \n",
    "        tuned_results = evaluate_model(best_pipeline, X_val, y_val, f\"{best_model_name} (Tuned)\")\n",
    "        print(f\"Tuned model - RMSE: {tuned_results['RMSE']:.2f}, R^2: {tuned_results['R^2']:.4f}\")\n",
    "        \n",
    "        model_results[f\"{best_model_name} (Tuned)\"] = tuned_results\n",
    "        model_results[f\"{best_model_name} (Tuned)\"]['pipeline'] = best_pipeline\n",
    "        \n",
    "        if tuned_results['RMSE'] < model_results[best_model_name]['RMSE']:\n",
    "            best_model_name = f\"{best_model_name} (Tuned)\"\n",
    "\n",
    "    return best_pipeline, param_grid, model_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of models to try\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'RandomForestRegressor': RandomForestRegressor(random_state=42),\n",
    "}\n",
    "\n",
    "preprocessor = define_preprocessor(claims_data)\n",
    "\n",
    "best_model_name, best_pipeline, model_results = execute_pipeline(models=models, preprocesser=preprocessor)\n",
    "# Tuning the best hyperparameter Tuning for the Best Model\n",
    "param_grid = {}\n",
    "\n",
    "if best_model_name == 'LinearRegression':\n",
    "    # regression model does not have hyperparameters to tune but best to keep this to keep track \n",
    "    pass\n",
    "elif best_model_name == 'Ridge':\n",
    "    param_grid = {\n",
    "        'model__alpha': [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "    }\n",
    "elif best_model_name == 'RandomForestRegressor':\n",
    "    param_grid = {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__max_depth': [None, 10, 20],\n",
    "        'model__min_samples_split': [2, 5],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline, param_grid, model_results = model_tuner(best_pipeline, param_grid, model_results)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_pipeline = model_results[best_model_name]['pipeline']\n",
    "test_results = evaluate_model(best_pipeline, X_test, y_test, best_model_name)\n",
    "print(f\"\\nTest set evaluation for {best_model_name}:\")\n",
    "print(f\"  RMSE: {test_results['RMSE']:.2f}\")\n",
    "print(f\"  MAE: {test_results['MAE']:.2f}\")\n",
    "print(f\"  R^2: {test_results['R^2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_settlement_plot(y_pred_test, y_test, best_model_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred_test, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Actual Settlement Values')\n",
    "    plt.ylabel('Predicted Settlement Values')\n",
    "    plt.title(f'Actual vs. Predicted Settlement Values ({best_model_name})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def residual_plot(y_test, y_pred_test):\n",
    "    # Calculate residuals\n",
    "    residuals = y_test - y_pred_test\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_pred_test, residuals, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted Settlement Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residual Plot')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = best_pipeline.predict(X_test)\n",
    "\n",
    "predicted_settlement_plot(y_test=y_test, y_pred_test=y_pred_test, best_model_name=best_model_name)\n",
    "residual_plot(y_test=y_test, y_pred_test=y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def important_features_calc(best_pipeline):\n",
    "    # Get feature names after preprocessing\n",
    "    if categorical_cols:\n",
    "        # Get feature names from the preprocessor\n",
    "        preprocessor = best_pipeline.named_steps['preprocessor']\n",
    "        feature_names = []\n",
    "        \n",
    "        # Get numerical feature names\n",
    "        if numerical_cols:\n",
    "            feature_names.extend(numerical_cols)\n",
    "        \n",
    "        # Get one-hot encoded feature names\n",
    "        if categorical_cols:\n",
    "            ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "            # Get category names from one-hot encoder\n",
    "            cat_features = ohe.get_feature_names_out(categorical_cols)\n",
    "            feature_names.extend(cat_features)\n",
    "    else:\n",
    "        feature_names = numerical_cols\n",
    "\n",
    "    # For tree-based models, we can get feature importance directly\n",
    "    if hasattr(best_pipeline.named_steps['model'], 'feature_importances_'):\n",
    "        importances = best_pipeline.named_steps['model'].feature_importances_\n",
    "        feature_imp = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_imp.head(20))\n",
    "        plt.title(f'Top 20 Feature Importances ({best_model_name})')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nTop 10 important features:\")\n",
    "        print(feature_imp.head(10))\n",
    "    else:\n",
    "        # For linear models, use coefficients\n",
    "        if hasattr(best_pipeline.named_steps['model'], 'coef_'):\n",
    "            coefs = best_pipeline.named_steps['model'].coef_\n",
    "            \n",
    "            if coefs.ndim == 1:  # For models with a 1D coefficient array\n",
    "                feature_imp = pd.DataFrame({\n",
    "                    'Feature': feature_names,\n",
    "                    'Coefficient': coefs\n",
    "                }).sort_values('Coefficient', key=abs, ascending=False)\n",
    "                \n",
    "                # Plot coefficient values\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                sns.barplot(x='Coefficient', y='Feature', data=feature_imp.head(20))\n",
    "                plt.title(f'Top 20 Feature Coefficients ({best_model_name})')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                print(\"\\nTop 10 features by coefficient magnitude:\")\n",
    "                print(feature_imp.head(10))\n",
    "        else:\n",
    "            # Permutation importance for models without built-in feature importance\n",
    "            print(\"\\nCalculating permutation importance...\")\n",
    "            perm_importance = permutation_importance(\n",
    "                best_pipeline, X_val, y_val, n_repeats=10, random_state=42, n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            feature_imp = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': perm_importance.importances_mean\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    return feature_imp\n",
    "\n",
    "def permutation_importance_plotting(feature_imp):\n",
    "    # Plot permutation importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_imp.head(20))\n",
    "    plt.title(f'Top 20 Permutation Importances ({best_model_name})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nTop 10 features by permutation importance:\")\n",
    "    print(feature_imp.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = important_features_calc(best_pipeline=best_pipeline)\n",
    "permutation_importance_plotting(feature_imp=feature_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_bias_plot(X_test_with_preds):\n",
    "    # Check for biases in predictions by protected characteristics\n",
    "    if 'Gender' in X_test.columns:\n",
    "        # Calculate average error by gender\n",
    "        gender_errors = X_test_with_preds.groupby('Gender')['Error'].mean()\n",
    "        \n",
    "        print(\"\\nAverage prediction error by gender:\")\n",
    "        print(gender_errors)\n",
    "\n",
    "        # Plot gender error comparison\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        gender_errors.plot(kind='bar')\n",
    "        plt.title('Average Prediction Error by Gender')\n",
    "        plt.ylabel('Mean Absolute Error')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Gender Column not found!\")\n",
    "    \n",
    "def age_bias_plot(X_test_with_preds):\n",
    "    # Check for biases by age\n",
    "    if 'DriverAge' in X_test.columns:\n",
    "        # Create age groups\n",
    "        X_test_with_preds['AgeGroup'] = pd.cut(X_test_with_preds['DriverAge'], \n",
    "                                            bins=[0, 30, 40, 50, 60, 70, 80],\n",
    "                                            labels=['Under 30', '30-40', '40-50', '50-60', '60-70', '70-80'])\n",
    "        \n",
    "        # Calculate average error by age group\n",
    "        age_group_errors = X_test_with_preds.groupby('AgeGroup', observed=False)['Error'].mean()\n",
    "\n",
    "        print(\"\\nAverage prediction error by age group:\")\n",
    "        print(age_group_errors)\n",
    "\n",
    "        # Plot age group error comparison\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        age_group_errors.plot(kind='bar')\n",
    "        plt.title('Average Prediction Error by Age Group')\n",
    "        plt.ylabel('Mean Absolute Error')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Age Column not found\")\n",
    "\n",
    "def bias_helper(X_test, y_test, y_pred_test):\n",
    "    # Add predictions to test data\n",
    "    X_test_with_preds = X_test.copy()\n",
    "    X_test_with_preds['Predicted'] = y_pred_test\n",
    "    X_test_with_preds['Actual'] = y_test.values\n",
    "    X_test_with_preds['Error'] = abs(y_test.values - y_pred_test)\n",
    "\n",
    "    return X_test_with_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_with_preds = bias_helper(X_test=X_test, y_test=y_test, y_pred_test=y_pred_test)\n",
    "\n",
    "age_bias_plot(X_test_with_preds=X_test_with_preds)\n",
    "gender_bias_plot(X_test_with_preds=X_test_with_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Random Forest with only the top ten most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = list(feature_imp.head(10)['Feature'])\n",
    "\n",
    "X_train_filtered = X_train[top_10]\n",
    "\n",
    "preprocessor = define_preprocessor(X_train_filtered)\n",
    "\n",
    "models = {'RandomForestRegressor': RandomForestRegressor(random_state=42),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all features have transferred over\n",
    "missing_columns = set(top_10) - set(X_train_filtered.columns)\n",
    "if missing_columns:\n",
    "    print(f\"Missing columns: {missing_columns}\")\n",
    "else:\n",
    "    print(\"All top 10 features are present in the filtered dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name, best_pipeline, model_results = execute_pipeline(models=models, preprocesser=preprocessor)\n",
    "\n",
    "param_grid = {\n",
    "        'model__n_estimators': [100, 200, 300, 400, 500],\n",
    "        'model__max_depth': [None, 10, 20, 30, 50],\n",
    "        'model__min_samples_split': [2, 5],\n",
    "    }\n",
    "\n",
    "best_pipeline, param_grid, model_results = model_tuner(best_pipeline, param_grid, model_results)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_pipeline = model_results[best_model_name]['pipeline']\n",
    "test_results = evaluate_model(best_pipeline, X_test, y_test, best_model_name)\n",
    "print(f\"\\nTest set evaluation for {best_model_name}:\")\n",
    "print(f\"  RMSE: {test_results['RMSE']:.2f}\")\n",
    "print(f\"  MAE: {test_results['MAE']:.2f}\")\n",
    "print(f\"  R^2: {test_results['R^2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_filename = 'limited_model.pkl'\n",
    "print(f\"Saving best model ({best_model_name}) to {best_model_filename}\")\n",
    "\n",
    "with open(best_model_filename, 'wb') as file:\n",
    "    pickle.dump(best_pipeline, file)\n",
    "    \n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'AccidentDate' in claims_data.columns and 'ClaimDate' in claims_data.columns:\n",
    "    claims_data['DaysBetweenAccidentAndClaim'] = claims_data['ClaimDate'] - claims_data['AccidentDate']\n",
    "\n",
    "categorical_cols, numerical_cols, target_col = identify_columns(claims_data=claims_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "claimData['LogSettlementValue'] = np.log1p(claimData[target_col])\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(claimData['LogSettlementValue'], kde=True)\n",
    "plt.title('Distribution of Log-Transformed Settlement Values')\n",
    "plt.xlabel('Log(Settlement Value + 1)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Update target column for log-transformed version\n",
    "log_transform = True\n",
    "target_col = 'LogSettlementValue'"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
